# -*- coding: utf-8 -*-
"""
Created on Wed Jan 24 08:49:30 2018

@author: SHEN
"""

import numpy as np

'''
参数分别是输入层 隐藏层 输出层的size
目的要输出初始化的 w1 b1 w2 b2
'''
def initialize_parameters(n_x, n_h, n_y):
    w1 = np.random.randn((n_h, n_x))
    b1 = np.zeros((n_x,1))    
    w2 = np.random.randn((n_y, n_h))
    b1 = np.zeros((n_h,1))
    
    assert(w1.shape == (n_h, n_x))
    assert(b1.shape == (n_x, 1))
    assert(w2.shape == (n_y, n_h))
    assert(b2.shape == (n_h, 1))
    parameters = {"w1": w1, 
                  "w2": w2,
                  "b1": b1,
                  "b2": b2}
    return parameters

#激活函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

'''
X 是(n_x, m)的样本
return a1 cache
'''
def forward_propagation(X, parameters):
    w1 = parameters["w1"]
    w2 = parameters["w2"]
    b1 = parameters["b1"]
    b2 = parameters["b2"]
    m = X.sharp[1]
   
    z1 = np.dot(w1, m) + b1
    z2 = np.dot(w2, m) + b2
    
    a1 = np.tanh(z1)
    a2 = sigmoid(z2)
    
    cache = {"z1": z1,
             "z2": z2,
             "a1": a1,
             "a2": a2
             }
    return a2, cache

def compute_cost(a2, Y, parameters):
    logprobs = np.multiply(np.log(a2),Y) + np.multiply(np.log(1 - a2),1 - Y)
    cost = - np.sum(logprobs) / m
    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect.
                                # E.g., turns [[17]] into 17
    assert(isinstance(cost, float))

    return cost

def backward_propagation(parameters, cache, X, Y):
    w1 = parameters["w1"]
    w2 = parameters["w2"]
    a1 = cache["a1"]
    a2 = cache["a2"]
    m = X.shape[1]
    
    dz2 = a2 - Y
    dw2 = np.dot(dz2, a1.T) / m
    db2 = np.sum(dz2, axis = 1, keepdims = True) / m
    dz1 = np.multiply(np.dot(w2.T, dz2) , (1 - np.power(a1, 2)))
    dw1 = np.dot(dz1, X.T) / m
    db1 = np.sum(dz1, axis = 1, keepdims = True) / m
    
    grads = {"dw1": dw1,
             "dw2":dw2,
             "db1":db1,
             "db2":db2}
    return grads
    
def update_parameters(parameters, grads, learning_rate = 1.2):
    w1 = parameters["w1"]
    w2 = parameters["w2"]
    b1 = parameters["b1"]
    b2 = parameters["b2"] 
    
    dw1 = grads["dw1"]
    dw2 = grads["dw2"]
    db1 = grads["db1"]
    db2 = grads["db2"] 
    
    w1 = w1 - learning_rate* dw1
    w2 = w2 - learning_rate* dw2
    b1 = b1 - learning_rate* db1
    b2 = b2 - learning_rate* db2
    
    parameters = {"w1": w1, 
                  "w2": w2,
                  "b1": b1,
                  "b2": b2}
    return parameters

'''
得出训练后的W b
'''
def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    for i in range(0, num_iteractions):       
        parameters = initialize_parameters(5,4,3)
        print(parameters["w1"].shape)
        A2, cache = forward_propagation(X, parameters)
        cost = compute_cost(A2, Y, parameters)
        grads = backward_propagation(parameters, cache, X, Y)
        parameters = update_parameters(parameters, grads)
    return parameters

def predict(parameters, X):
    A2, cache = forward_propagation(X, parameters)
    predictions = np.around(A2)
    return predictions
